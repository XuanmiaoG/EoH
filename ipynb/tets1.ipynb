{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf476a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load training data from: data/all_data_train.pkl\n",
      "Attempting to load test data from: data/all_data_test.pkl\n",
      "Training data loaded successfully.\n",
      "Test data loaded successfully.\n",
      "\n",
      "--- Verifying Structure Across All Settings ---\n",
      "\n",
      "Training Data Settings:\n",
      "  ('uniform', 5): keys = ['peaks', 'misreports']\n",
      "  ('normal', 5): keys = ['peaks', 'misreports']\n",
      "  ('beta1', 5): keys = ['peaks', 'misreports']\n",
      "  ('beta2', 5): keys = ['peaks', 'misreports']\n",
      "  ('uniform', 9): keys = ['peaks', 'misreports']\n",
      "  ('normal', 9): keys = ['peaks', 'misreports']\n",
      "  ('beta1', 9): keys = ['peaks', 'misreports']\n",
      "  ('beta2', 9): keys = ['peaks', 'misreports']\n",
      "  ('uniform', 10): keys = ['peaks', 'misreports']\n",
      "  ('normal', 10): keys = ['peaks', 'misreports']\n",
      "  ('beta1', 10): keys = ['peaks', 'misreports']\n",
      "  ('beta2', 10): keys = ['peaks', 'misreports']\n",
      "  ('uniform', 100): keys = ['peaks', 'misreports']\n",
      "  ('normal', 100): keys = ['peaks', 'misreports']\n",
      "  ('beta1', 100): keys = ['peaks', 'misreports']\n",
      "  ('beta2', 100): keys = ['peaks', 'misreports']\n",
      "\n",
      "Test Data Settings:\n",
      "  ('uniform', 5): keys = ['peaks', 'misreports']\n",
      "  ('normal', 5): keys = ['peaks', 'misreports']\n",
      "  ('beta1', 5): keys = ['peaks', 'misreports']\n",
      "  ('beta2', 5): keys = ['peaks', 'misreports']\n",
      "  ('uniform', 9): keys = ['peaks', 'misreports']\n",
      "  ('normal', 9): keys = ['peaks', 'misreports']\n",
      "  ('beta1', 9): keys = ['peaks', 'misreports']\n",
      "  ('beta2', 9): keys = ['peaks', 'misreports']\n",
      "  ('uniform', 10): keys = ['peaks', 'misreports']\n",
      "  ('normal', 10): keys = ['peaks', 'misreports']\n",
      "  ('beta1', 10): keys = ['peaks', 'misreports']\n",
      "  ('beta2', 10): keys = ['peaks', 'misreports']\n",
      "  ('uniform', 100): keys = ['peaks', 'misreports']\n",
      "  ('normal', 100): keys = ['peaks', 'misreports']\n",
      "  ('beta1', 100): keys = ['peaks', 'misreports']\n",
      "  ('beta2', 100): keys = ['peaks', 'misreports']\n",
      "\n",
      "--- Inspect Training Setting ('uniform', 5) ---\n",
      "Inner keys: ['peaks', 'misreports']\n",
      "Peaks shape: (1000, 5)\n",
      "First 3 peak samples:\n",
      " [[0.6605843  0.9415198  0.3298248  0.3723629  0.2944604 ]\n",
      " [0.3763239  0.3456326  0.12844406 0.5728453  0.61359185]\n",
      " [0.5682903  0.3775263  0.6112781  0.44039837 0.80327964]]\n",
      "Weights not found or wrong type.\n",
      "Misreports shape: (10000, 5)\n",
      "\n",
      "--- Inspect Test Setting ('normal', 10) ---\n",
      "Inner keys: ['peaks', 'misreports']\n",
      "Test peaks shape: (1000, 10)\n",
      "First 3 test peak samples:\n",
      " [[0.38848403 0.597602   0.7925984  0.645824   0.44227484 0.35630843\n",
      "  0.35852942 0.23251596 0.47817558 0.4732429 ]\n",
      " [0.44615188 0.509723   0.4379889  0.551949   0.49355078 0.38903293\n",
      "  0.39929724 0.45019555 0.5513439  0.41886795]\n",
      " [0.66717994 0.47771087 0.42489845 0.24032965 0.6206831  0.40246382\n",
      "  0.44066188 0.41847557 0.3579344  0.5059821 ]]\n",
      "Test weights not found or wrong type.\n",
      "\n",
      "--- End of Data Exploration ---\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Demonstrates loading and accessing the facility location problem data.\n",
    "Handles potential nested dictionary structure and verifies structure across all settings.\n",
    "Also inspects agent weights alongside peaks and misreports.\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_DIR = 'data'\n",
    "TRAIN_DATA_FILE = os.path.join(DATA_DIR, 'all_data_train.pkl')\n",
    "TEST_DATA_FILE = os.path.join(DATA_DIR, 'all_data_test.pkl')\n",
    "\n",
    "# --- Load Data ---\n",
    "print(f\"Attempting to load training data from: {TRAIN_DATA_FILE}\")\n",
    "print(f\"Attempting to load test data from: {TEST_DATA_FILE}\")\n",
    "\n",
    "data_train = None\n",
    "data_test = None\n",
    "\n",
    "# Load training data\n",
    "try:\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        print(f\"WARNING: Data directory '{DATA_DIR}' not found. Creating it.\")\n",
    "        os.makedirs(DATA_DIR)\n",
    "        print(f\"Please place '{os.path.basename(TRAIN_DATA_FILE)}' and '{os.path.basename(TEST_DATA_FILE)}' in '{DATA_DIR}'.\")\n",
    "    with open(TRAIN_DATA_FILE, 'rb') as f:\n",
    "        data_train = pickle.load(f)\n",
    "    print(\"Training data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Training data file not found at {TRAIN_DATA_FILE}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load training data. {e}\")\n",
    "\n",
    "# Load test data\n",
    "try:\n",
    "    with open(TEST_DATA_FILE, 'rb') as f:\n",
    "        data_test = pickle.load(f)\n",
    "    print(\"Test data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Test data file not found at {TEST_DATA_FILE}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load test data. {e}\")\n",
    "\n",
    "# --- Verify Structure Across All Settings ---\n",
    "print(\"\\n--- Verifying Structure Across All Settings ---\")\n",
    "if data_train:\n",
    "    print(\"\\nTraining Data Settings:\")\n",
    "    for setting, content in data_train.items():\n",
    "        if isinstance(content, dict):\n",
    "            print(f\"  {setting}: keys = {list(content.keys())}\")\n",
    "        else:\n",
    "            print(f\"  {setting}: unexpected type {type(content)}\")\n",
    "else:\n",
    "    print(\"No training data.\")\n",
    "\n",
    "if data_test:\n",
    "    print(\"\\nTest Data Settings:\")\n",
    "    for setting, content in data_test.items():\n",
    "        if isinstance(content, dict):\n",
    "            print(f\"  {setting}: keys = {list(content.keys())}\")\n",
    "        else:\n",
    "            print(f\"  {setting}: unexpected type {type(content)}\")\n",
    "else:\n",
    "    print(\"No test data.\")\n",
    "\n",
    "# --- Inspect a Specific Setting: training ---\n",
    "if data_train:\n",
    "    example_setting = ('uniform', 5)\n",
    "    print(f\"\\n--- Inspect Training Setting {example_setting} ---\")\n",
    "    if example_setting in data_train:\n",
    "        setting_data = data_train[example_setting]\n",
    "        print(\"Inner keys:\", list(setting_data.keys()))\n",
    "\n",
    "        # Peaks\n",
    "        peaks = setting_data.get('peaks', None)\n",
    "        if isinstance(peaks, np.ndarray):\n",
    "            print(\"Peaks shape:\", peaks.shape)\n",
    "            print(\"First 3 peak samples:\\n\", peaks[:3])\n",
    "        else:\n",
    "            print(\"Peaks not found or wrong type.\")\n",
    "\n",
    "        # Weights\n",
    "        weights = setting_data.get('weights', None)\n",
    "        if isinstance(weights, np.ndarray):\n",
    "            print(\"Weights shape:\", weights.shape)\n",
    "            if weights.ndim == 1:\n",
    "                print(\"Agent weights (constant):\", weights)\n",
    "            elif weights.ndim == 2:\n",
    "                print(\"First 3 samples' weights:\\n\", weights[:3])\n",
    "        else:\n",
    "            print(\"Weights not found or wrong type.\")\n",
    "\n",
    "        # Misreports\n",
    "        misr = setting_data.get('misreports', None)\n",
    "        if isinstance(misr, np.ndarray):\n",
    "            print(\"Misreports shape:\", misr.shape)\n",
    "            if misr.ndim >= 3:\n",
    "                print(\"Misreports for agent 0 of sample 0:\", misr[0, 0, :])\n",
    "        else:\n",
    "            print(\"Misreports not found or wrong type.\")\n",
    "    else:\n",
    "        print(\"Setting not found in training data.\")\n",
    "\n",
    "# --- Inspect a Specific Setting: test ---\n",
    "if data_test:\n",
    "    example_test = ('normal', 10)\n",
    "    print(f\"\\n--- Inspect Test Setting {example_test} ---\")\n",
    "    if example_test in data_test:\n",
    "        setting_data = data_test[example_test]\n",
    "        print(\"Inner keys:\", list(setting_data.keys()))\n",
    "\n",
    "        # Peaks\n",
    "        peaks = setting_data.get('peaks', None)\n",
    "        if isinstance(peaks, np.ndarray):\n",
    "            print(\"Test peaks shape:\", peaks.shape)\n",
    "            print(\"First 3 test peak samples:\\n\", peaks[:3])\n",
    "        else:\n",
    "            print(\"Test peaks not found or wrong type.\")\n",
    "\n",
    "        # Weights\n",
    "        weights = setting_data.get('weights', None)\n",
    "        if isinstance(weights, np.ndarray):\n",
    "            print(\"Test weights shape:\", weights.shape)\n",
    "            if weights.ndim == 1:\n",
    "                print(\"Test agent weights (constant):\", weights)\n",
    "            elif weights.ndim == 2:\n",
    "                print(\"First 3 test samples' weights:\\n\", weights[:3])\n",
    "        else:\n",
    "            print(\"Test weights not found or wrong type.\")\n",
    "    else:\n",
    "        print(\"Setting not found in test data.\")\n",
    "else:\n",
    "    print(\"No test data to inspect.\")\n",
    "\n",
    "print(\"\\n--- End of Data Exploration ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb5c4fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for setting: (uniform, 5)\n",
      "\n",
      "Successfully loaded test peaks. Shape: (1000, 5)\n",
      "Successfully loaded test misreports. Shape: (10000, 5)\n",
      "\n",
      "--- Evaluating Baselines on Test Set ---\n",
      "Setting: Distribution=uniform, Agents=5, Facilities=2\n",
      "Processed 100/1000 instances...\n",
      "Processed 200/1000 instances...\n",
      "Processed 300/1000 instances...\n",
      "Processed 400/1000 instances...\n",
      "Processed 500/1000 instances...\n",
      "Processed 600/1000 instances...\n",
      "Processed 700/1000 instances...\n",
      "Processed 800/1000 instances...\n",
      "Processed 900/1000 instances...\n",
      "Processed 1000/1000 instances...\n",
      "\n",
      "Evaluation finished in 0.27 seconds.\n",
      "\n",
      "--- Average Results on Test Set ---\n",
      "Percentile (Median Partition):\n",
      "  Avg. Social Cost: 0.088011\n",
      "  Avg. Max Regret:  0.000000\n",
      "Best Dictatorial:\n",
      "  Avg. Social Cost: 0.200382\n",
      "  Avg. Max Regret:  0.000000\n",
      "Constant (Evenly Spaced):\n",
      "  Avg. Social Cost: 0.138031\n",
      "  Avg. Max Regret:  0.000000\n",
      "\n",
      "--- End of Baseline Evaluation ---\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Implements and evaluates baseline mechanisms for the facility location problem.\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_DIR = 'data'\n",
    "TRAIN_DATA_FILE = os.path.join(DATA_DIR, 'all_data_train.pkl')\n",
    "TEST_DATA_FILE = os.path.join(DATA_DIR, 'all_data_test.pkl')\n",
    "PEAK_DATA_KEY = 'peaks' # Key for accessing peak data in the loaded dictionary\n",
    "MISREPORT_DATA_KEY = 'misreports' # Key for accessing misreport data\n",
    "\n",
    "# --- Evaluation Setting ---\n",
    "# Choose the problem setting and number of facilities (K) to evaluate\n",
    "DISTRIBUTION = 'uniform'\n",
    "NUM_AGENTS = 5\n",
    "K_FACILITIES = 2 # Number of facilities to locate\n",
    "\n",
    "# Define agent weights (uniform weights for this example)\n",
    "# For weighted scenarios from the paper, use e.g., [5, 1, 1, 1, 1] for n=5\n",
    "AGENT_WEIGHTS = np.ones(NUM_AGENTS)\n",
    "# Normalize weights\n",
    "AGENT_WEIGHTS = AGENT_WEIGHTS / np.sum(AGENT_WEIGHTS)\n",
    "\n",
    "# Regret calculation parameters\n",
    "# Assume misreports structure: (num_samples, num_agents, num_misreports_per_agent)\n",
    "# If structure is different, adjust calculate_regret function\n",
    "NUM_MISREPORTS_PER_AGENT = 10 # From LLMMech paper Section 4.1\n",
    "\n",
    "# --- Load Data ---\n",
    "print(f\"Loading data for setting: ({DISTRIBUTION}, {NUM_AGENTS})\")\n",
    "\n",
    "data_train = None\n",
    "data_test = None\n",
    "train_peaks = None\n",
    "train_misreports = None\n",
    "test_peaks = None\n",
    "test_misreports = None\n",
    "\n",
    "try:\n",
    "    with open(TRAIN_DATA_FILE, 'rb') as f:\n",
    "        data_train = pickle.load(f)\n",
    "    setting_key = (DISTRIBUTION, NUM_AGENTS)\n",
    "    if setting_key in data_train and isinstance(data_train[setting_key], dict):\n",
    "        train_peaks = data_train[setting_key].get(PEAK_DATA_KEY)\n",
    "        train_misreports = data_train[setting_key].get(MISREPORT_DATA_KEY)\n",
    "        if train_peaks is None:\n",
    "             print(f\"ERROR: Key '{PEAK_DATA_KEY}' not found in training data for {setting_key}\")\n",
    "        # Add check for misreports if needed for training phase (e.g., finding best constant)\n",
    "    else:\n",
    "        print(f\"ERROR: Setting {setting_key} not found or not a dict in training data.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Training data file not found at {TRAIN_DATA_FILE}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load training data. {e}\")\n",
    "\n",
    "try:\n",
    "    with open(TEST_DATA_FILE, 'rb') as f:\n",
    "        data_test = pickle.load(f)\n",
    "    setting_key = (DISTRIBUTION, NUM_AGENTS)\n",
    "    if setting_key in data_test and isinstance(data_test[setting_key], dict):\n",
    "        test_peaks = data_test[setting_key].get(PEAK_DATA_KEY)\n",
    "        test_misreports = data_test[setting_key].get(MISREPORT_DATA_KEY)\n",
    "        if test_peaks is None:\n",
    "             print(f\"ERROR: Key '{PEAK_DATA_KEY}' not found in test data for {setting_key}\")\n",
    "        if test_misreports is None:\n",
    "             print(f\"ERROR: Key '{MISREPORT_DATA_KEY}' not found in test data for {setting_key}\")\n",
    "             print(\"Regret calculation will not be possible.\")\n",
    "    else:\n",
    "        print(f\"ERROR: Setting {setting_key} not found or not a dict in test data.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Test data file not found at {TEST_DATA_FILE}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load test data. {e}\")\n",
    "\n",
    "# Proceed only if test data is loaded\n",
    "if test_peaks is not None:\n",
    "    print(f\"\\nSuccessfully loaded test peaks. Shape: {test_peaks.shape}\")\n",
    "    if test_misreports is not None:\n",
    "         print(f\"Successfully loaded test misreports. Shape: {test_misreports.shape}\") # Print shape to verify structure\n",
    "         # Expected shape e.g., (1000, 5, 10) for 1000 samples, 5 agents, 10 misreports each\n",
    "         # Adjust NUM_MISREPORTS_PER_AGENT if the shape differs\n",
    "\n",
    "    # --- Helper Functions ---\n",
    "\n",
    "    def calculate_social_cost(agent_peaks, facility_locations, weights):\n",
    "        \"\"\"Calculates the weighted social cost for one instance.\"\"\"\n",
    "        if not isinstance(facility_locations, (list, np.ndarray)) or len(facility_locations) == 0:\n",
    "            return np.inf # Invalid locations\n",
    "        if not isinstance(agent_peaks, (list, np.ndarray)):\n",
    "             return np.inf # Invalid peaks\n",
    "\n",
    "        agent_peaks = np.array(agent_peaks)\n",
    "        facility_locations = np.array(facility_locations)\n",
    "        weights = np.array(weights)\n",
    "\n",
    "        # Calculate distance from each agent to their nearest facility\n",
    "        # Expand dims for broadcasting: agent_peaks (n, 1), facility_locations (k,) -> distances (n, k)\n",
    "        distances = np.abs(agent_peaks[:, np.newaxis] - facility_locations)\n",
    "        min_distances = np.min(distances, axis=1) # Shape (n,)\n",
    "\n",
    "        # Calculate weighted average cost\n",
    "        weighted_cost = np.sum(min_distances * weights) # Uses normalized weights implicitly\n",
    "        # If weights were not normalized, divide by np.sum(weights)\n",
    "        return weighted_cost\n",
    "\n",
    "    def calculate_regret(mechanism_func, instance_idx, true_peaks_all, misreports_all, weights, k_facilities):\n",
    "        \"\"\"\n",
    "        Calculates the maximum regret for a given mechanism on a specific instance.\n",
    "        Assumes misreports_all shape is (num_samples, num_agents, num_misreports)\n",
    "        \"\"\"\n",
    "        if misreports_all is None:\n",
    "            # print(\"Misreport data not available, skipping regret calculation.\")\n",
    "            return 0.0 # Cannot calculate regret\n",
    "\n",
    "        true_peaks_instance = true_peaks_all[instance_idx]\n",
    "        misreports_instance = misreports_all[instance_idx] # Shape (num_agents, num_misreports)\n",
    "\n",
    "        # 1. Calculate cost with true reports\n",
    "        true_locations = mechanism_func(true_peaks_instance, k_facilities, weights)\n",
    "        true_cost_per_agent = np.min(np.abs(true_peaks_instance[:, np.newaxis] - np.array(true_locations)), axis=1)\n",
    "\n",
    "        max_regret_instance = 0.0\n",
    "\n",
    "        # 2. Iterate through each agent potentially misreporting\n",
    "        for agent_i in range(len(true_peaks_instance)):\n",
    "            max_gain_agent_i = 0.0\n",
    "            # 3. Iterate through each possible misreport for that agent\n",
    "            # Ensure misreports_instance has the expected dimensions\n",
    "            if misreports_instance.ndim == 2 and misreports_instance.shape[0] == len(true_peaks_instance):\n",
    "                 for misreport_j in range(misreports_instance.shape[1]):\n",
    "                    misreported_peak = misreports_instance[agent_i, misreport_j]\n",
    "                    # Create the peak profile with agent_i misreporting\n",
    "                    misreport_profile = np.copy(true_peaks_instance)\n",
    "                    misreport_profile[agent_i] = misreported_peak\n",
    "\n",
    "                    # 4. Run mechanism with the misreported profile\n",
    "                    misreport_locations = mechanism_func(misreport_profile, k_facilities, weights)\n",
    "\n",
    "                    # 5. Calculate agent_i's cost *using their true peak* but with the new locations\n",
    "                    cost_with_misreport = np.min(np.abs(true_peaks_instance[agent_i] - np.array(misreport_locations)))\n",
    "\n",
    "                    # 6. Calculate gain (true cost - cost when misreporting)\n",
    "                    gain = true_cost_per_agent[agent_i] - cost_with_misreport\n",
    "\n",
    "                    # 7. Track max gain for this agent\n",
    "                    if gain > max_gain_agent_i:\n",
    "                        max_gain_agent_i = gain\n",
    "            else:\n",
    "                 # Handle cases where misreport structure might be different or missing\n",
    "                 # print(f\"Warning: Unexpected misreport structure for instance {instance_idx}, agent {agent_i}. Shape: {misreports_instance.shape}\")\n",
    "                 pass # Skip regret calculation for this agent/instance if structure is wrong\n",
    "\n",
    "            # 8. Track max regret across all agents for this instance\n",
    "            if max_gain_agent_i > max_regret_instance:\n",
    "                max_regret_instance = max_gain_agent_i\n",
    "\n",
    "        return max_regret_instance\n",
    "\n",
    "\n",
    "    # --- Baseline Mechanism Implementations ---\n",
    "\n",
    "    def percentile_rule(agent_peaks, k_facilities, weights=None):\n",
    "        \"\"\"Implements a fixed percentile rule (median-based partitioning).\"\"\"\n",
    "        n_agents = len(agent_peaks)\n",
    "        sorted_peaks = np.sort(agent_peaks)\n",
    "\n",
    "        if k_facilities == 0:\n",
    "            return []\n",
    "        if k_facilities == 1:\n",
    "            return [np.median(sorted_peaks)]\n",
    "        if k_facilities >= n_agents: # Place facility at each unique peak\n",
    "             return sorted(list(np.unique(sorted_peaks)))[:k_facilities]\n",
    "\n",
    "        # Partition agents into K groups and find median of each\n",
    "        locations = []\n",
    "        indices = np.linspace(0, n_agents, k_facilities + 1, dtype=int)\n",
    "        for i in range(k_facilities):\n",
    "            group = sorted_peaks[indices[i]:indices[i+1]]\n",
    "            if len(group) > 0:\n",
    "                locations.append(np.median(group))\n",
    "            # Handle potential empty groups if k is large (though covered by k>=n check)\n",
    "            # else: locations.append(np.median(sorted_peaks)) # Fallback, unlikely needed\n",
    "\n",
    "        # Ensure K locations are returned, handle edge cases if partitioning failed\n",
    "        while len(locations) < k_facilities and len(locations) > 0:\n",
    "             locations.append(locations[-1]) # Duplicate last location\n",
    "        if not locations: # If K>0 but partitioning failed completely\n",
    "             locations = [np.median(sorted_peaks)] * k_facilities\n",
    "\n",
    "\n",
    "        return sorted(locations) # Return sorted locations\n",
    "\n",
    "    def dictatorial_rule(agent_peaks, k_facilities, weights, dictator_index):\n",
    "        \"\"\"Places all K facilities at the dictator's peak.\"\"\"\n",
    "        if dictator_index < 0 or dictator_index >= len(agent_peaks):\n",
    "            print(f\"Warning: Invalid dictator index {dictator_index}\")\n",
    "            return [np.median(agent_peaks)] * k_facilities # Fallback\n",
    "        return [agent_peaks[dictator_index]] * k_facilities\n",
    "\n",
    "    def best_dictatorial_rule(agent_peaks, k_facilities, weights):\n",
    "        \"\"\"Finds the best dictator for a specific instance.\"\"\"\n",
    "        n_agents = len(agent_peaks)\n",
    "        best_cost = np.inf\n",
    "        best_locations = []\n",
    "\n",
    "        for i in range(n_agents):\n",
    "            locations = dictatorial_rule(agent_peaks, k_facilities, weights, i)\n",
    "            cost = calculate_social_cost(agent_peaks, locations, weights)\n",
    "            if cost < best_cost:\n",
    "                best_cost = cost\n",
    "                best_locations = locations\n",
    "\n",
    "        # Handle case where no valid dictator found (shouldn't happen if peaks are valid)\n",
    "        if not best_locations:\n",
    "             median_loc = np.median(agent_peaks)\n",
    "             best_locations = [median_loc] * k_facilities\n",
    "\n",
    "        return best_locations\n",
    "\n",
    "    def constant_rule(agent_peaks, k_facilities, weights=None):\n",
    "        \"\"\"Uses fixed, evenly spaced locations.\"\"\"\n",
    "        if k_facilities == 0:\n",
    "            return []\n",
    "        # Simple fixed locations: 1/(K+1), 2/(K+1), ..., K/(K+1)\n",
    "        locations = [ (i + 1.0) / (k_facilities + 1.0) for i in range(k_facilities) ]\n",
    "        return locations\n",
    "\n",
    "    # --- Evaluation Loop ---\n",
    "    print(f\"\\n--- Evaluating Baselines on Test Set ---\")\n",
    "    print(f\"Setting: Distribution={DISTRIBUTION}, Agents={NUM_AGENTS}, Facilities={K_FACILITIES}\")\n",
    "\n",
    "    baseline_results = {\n",
    "        \"Percentile (Median Partition)\": {\"costs\": [], \"regrets\": []},\n",
    "        \"Best Dictatorial\": {\"costs\": [], \"regrets\": []},\n",
    "        \"Constant (Evenly Spaced)\": {\"costs\": [], \"regrets\": []},\n",
    "    }\n",
    "\n",
    "    num_test_samples = test_peaks.shape[0]\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(num_test_samples):\n",
    "        instance_peaks = test_peaks[i]\n",
    "\n",
    "        # Evaluate Percentile Rule\n",
    "        perc_locs = percentile_rule(instance_peaks, K_FACILITIES)\n",
    "        perc_cost = calculate_social_cost(instance_peaks, perc_locs, AGENT_WEIGHTS)\n",
    "        perc_regret = calculate_regret(percentile_rule, i, test_peaks, test_misreports, AGENT_WEIGHTS, K_FACILITIES)\n",
    "        baseline_results[\"Percentile (Median Partition)\"][\"costs\"].append(perc_cost)\n",
    "        baseline_results[\"Percentile (Median Partition)\"][\"regrets\"].append(perc_regret)\n",
    "\n",
    "        # Evaluate Best Dictatorial Rule\n",
    "        bdict_locs = best_dictatorial_rule(instance_peaks, K_FACILITIES, AGENT_WEIGHTS)\n",
    "        bdict_cost = calculate_social_cost(instance_peaks, bdict_locs, AGENT_WEIGHTS)\n",
    "        # Regret for best dictatorial is tricky: which dictator rule was chosen *for this instance*?\n",
    "        # We need to pass the *specific* best dictator index found for this instance to calculate_regret,\n",
    "        # or recalculate regret based on the *best_dictatorial_rule* function itself.\n",
    "        # For simplicity here, we recalculate based on the function.\n",
    "        bdict_regret = calculate_regret(best_dictatorial_rule, i, test_peaks, test_misreports, AGENT_WEIGHTS, K_FACILITIES)\n",
    "        baseline_results[\"Best Dictatorial\"][\"costs\"].append(bdict_cost)\n",
    "        baseline_results[\"Best Dictatorial\"][\"regrets\"].append(bdict_regret)\n",
    "\n",
    "        # Evaluate Constant Rule\n",
    "        const_locs = constant_rule(instance_peaks, K_FACILITIES)\n",
    "        const_cost = calculate_social_cost(instance_peaks, const_locs, AGENT_WEIGHTS)\n",
    "        const_regret = calculate_regret(constant_rule, i, test_peaks, test_misreports, AGENT_WEIGHTS, K_FACILITIES)\n",
    "        baseline_results[\"Constant (Evenly Spaced)\"][\"costs\"].append(const_cost)\n",
    "        baseline_results[\"Constant (Evenly Spaced)\"][\"regrets\"].append(const_regret)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processed {i+1}/{num_test_samples} instances...\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nEvaluation finished in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    # --- Report Results ---\n",
    "    print(\"\\n--- Average Results on Test Set ---\")\n",
    "    for name, results in baseline_results.items():\n",
    "        avg_cost = np.mean(results[\"costs\"]) if results[\"costs\"] else np.nan\n",
    "        avg_regret = np.mean(results[\"regrets\"]) if results[\"regrets\"] else np.nan\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Avg. Social Cost: {avg_cost:.6f}\")\n",
    "        if test_misreports is not None:\n",
    "             print(f\"  Avg. Max Regret:  {avg_regret:.6f}\")\n",
    "        else:\n",
    "             print(\"  Avg. Max Regret:  Not calculated (misreports data missing)\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nCannot proceed with evaluation as test data failed to load.\")\n",
    "\n",
    "print(\"\\n--- End of Baseline Evaluation ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
