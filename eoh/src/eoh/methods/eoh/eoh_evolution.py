import json
import time
from ...llm.interface_LLM import InterfaceLLM  # Uncomment and update as needed


class Evolution:
    def __init__(
        self,
        api_endpoint: str | None,
        api_key: str | None,
        model_LLM: str | None,
        llm_use_local: bool,
        llm_local_url: str | None,
        debug_mode: bool,
        prompts: object,
        **kwargs: object,
    ) -> None:
        """
        Handles the generation of new algorithm code strings via a Large Language Model (LLM),
        using different prompting strategies (e.g., i1, e1, m1, etc.).

        Args:
            api_endpoint: (Optional) Remote LLM API endpoint.
            api_key: (Optional) API key for remote LLM usage.
            model_LLM: (Optional) Name or identifier of the LLM to use.
            llm_use_local: If True, use a local LLM server instead of a remote one.
            llm_local_url: (Optional) URL for the local LLM server.
            debug_mode: If True, prints additional debug information to the console.
            prompts: An object that provides various prompt strings (task, func_name, etc.).
            **kwargs: Additional parameters that may be needed by the LLM interface.

        Attributes:
            prompt_task (str): The main task or objective (e.g., "design an algorithm").
            prompt_func_name (str): The name of the Python function to be generated by the LLM.
            prompt_func_inputs (list[str]): The list of input parameter names for the function.
            prompt_func_outputs (list[str]): The list of output parameter names for the function.
            prompt_inout_inf (str): Additional prompt text about input-output formats.
            prompt_other_inf (str): Other auxiliary information for the prompt.
            joined_inputs (str): Comma-separated string of input parameter names.
            joined_outputs (str): Comma-separated string of output parameter names.
            interface_llm (InterfaceLLM): The interface to the local or remote LLM.
        """
        self.prompt_task: str = prompts.get_task()
        self.prompt_func_name: str = prompts.get_func_name()
        self.prompt_func_inputs: list[str] = prompts.get_func_inputs()
        self.prompt_func_outputs: list[str] = prompts.get_func_outputs()
        self.prompt_inout_inf: str = prompts.get_inout_inf()
        self.prompt_other_inf: str = prompts.get_other_inf()

        # Join inputs/outputs with commas if more than one
        if len(self.prompt_func_inputs) > 1:
            self.joined_inputs: str = ", ".join(
                "'" + s + "'" for s in self.prompt_func_inputs
            )
        else:
            self.joined_inputs: str = "'" + self.prompt_func_inputs[0] + "'"

        if len(self.prompt_func_outputs) > 1:
            self.joined_outputs: str = ", ".join(
                "'" + s + "'" for s in self.prompt_func_outputs
            )
        else:
            self.joined_outputs: str = "'" + self.prompt_func_outputs[0] + "'"

        self.api_endpoint: str | None = api_endpoint
        self.api_key: str | None = api_key
        self.model_LLM: str | None = model_LLM
        self.debug_mode: bool = debug_mode

        self.interface_llm = InterfaceLLM(
            self.api_endpoint,
            self.api_key,
            self.model_LLM,
            llm_use_local,
            llm_local_url,
            self.debug_mode,
        )

    ############################################################################
    # Prompt Generators (requesting JSON with keys "algorithm" and "code")
    ############################################################################

    def get_prompt_i1(self) -> str:
        """
        Build the prompt content for operator 'i1' (e.g., initial algorithm design).

        Returns:
            A string containing the prompt instructions for the LLM.
        """
        # We explicitly request valid JSON with 'algorithm' and 'code'
        prompt_content = (
            self.prompt_task
            + "\n"
            + "Please return your answer in **valid JSON** format with the following structure:\n"
            + """
{
  "algorithm": "...", 
  "code": "..."
}
"""
            + "Important:\n"
            + " - 'algorithm' must be a single-sentence description in curly braces\n"
            + " - 'code' must be the entire Python code\n"
            + " - No extra keys, no commentary outside the JSON.\n\n"
            + "Now describing your new algorithm:\n"
            + "1. Provide the single-sentence description in curly braces.\n"
            + "2. Implement the function named "
            + self.prompt_func_name
            + ", which accepts "
            + str(len(self.prompt_func_inputs))
            + " input(s): "
            + self.joined_inputs
            + ", returning "
            + str(len(self.prompt_func_outputs))
            + " output(s): "
            + self.joined_outputs
            + ". "
            + self.prompt_inout_inf
            + " "
            + self.prompt_other_inf
            + "\n"
            + "No further explanation. Output must be **valid JSON** with keys 'algorithm' and 'code'."
        )
        return prompt_content

    def get_prompt_e1(self, indivs: list[dict[str, str]]) -> str:
        """
        Build the prompt content for operator 'e1'.
        Uses a set of existing algorithms (indivs) to create something totally different.
        """
        prompt_indiv = ""
        for i, indiv in enumerate(indivs):
            prompt_indiv += (
                f"No.{i + 1} algorithm + code:\n"
                + indiv["algorithm"]
                + "\n"
                + indiv["code"]
                + "\n"
            )

        prompt_content = (
            self.prompt_task
            + "\n"
            + "Here are some existing algorithms:\n"
            + prompt_indiv
            + "Please create a NEW algorithm that is totally different.\n\n"
            + "Return your answer in **valid JSON** format:\n"
            + """
{
  "algorithm": "...", 
  "code": "..."
}
"""
            + "Implement a function named "
            + self.prompt_func_name
            + " with "
            + str(len(self.prompt_func_inputs))
            + " inputs: "
            + self.joined_inputs
            + ", returning "
            + str(len(self.prompt_func_outputs))
            + " outputs: "
            + self.joined_outputs
            + ". "
            + self.prompt_inout_inf
            + " "
            + self.prompt_other_inf
            + "\nNo further explanation. Must be valid JSON with 'algorithm' and 'code'."
        )
        return prompt_content

    def get_prompt_e2(self, indivs: list[dict[str, str]]) -> str:
        """
        Build the prompt content for operator 'e2'.
        Motivates a new algorithm based on the common backbone of existing ones.
        """
        prompt_indiv = ""
        for i, indiv in enumerate(indivs):
            prompt_indiv += (
                f"No.{i + 1} algorithm + code:\n"
                + indiv["algorithm"]
                + "\n"
                + indiv["code"]
                + "\n"
            )

        prompt_content = (
            self.prompt_task
            + "\n"
            + "We have these existing algorithms:\n"
            + prompt_indiv
            + "Please create a NEW algorithm that is different yet motivated by them.\n\n"
            + "Return your answer in **valid JSON** format:\n"
            + """
{
  "algorithm": "...",
  "code": "..."
}
"""
            + "Implement a function named "
            + self.prompt_func_name
            + " with "
            + str(len(self.prompt_func_inputs))
            + " inputs: "
            + self.joined_inputs
            + ", returning "
            + str(len(self.prompt_func_outputs))
            + " outputs: "
            + self.joined_outputs
            + ". "
            + self.prompt_inout_inf
            + " "
            + self.prompt_other_inf
            + "\nNo extra explanation. Must be valid JSON with 'algorithm' and 'code'."
        )
        return prompt_content

    def get_prompt_m1(self, indiv1: dict[str, str]) -> str:
        """
        Build the prompt content for operator 'm1'.
        Creates a new algorithm as a modified version of the given one.
        """
        prompt_content = (
            self.prompt_task
            + "\n"
            + "Current algorithm + code:\n"
            + indiv1["algorithm"]
            + "\n"
            + indiv1["code"]
            + "\n\n"
            + "Please create a new algorithm in a different form but as a modification of the above.\n\n"
            + "Return your answer in **valid JSON** format:\n"
            + """
{
  "algorithm": "...",
  "code": "..."
}
"""
            + "Implement function "
            + self.prompt_func_name
            + " with "
            + str(len(self.prompt_func_inputs))
            + " inputs: "
            + self.joined_inputs
            + ", returning "
            + str(len(self.prompt_func_outputs))
            + " outputs: "
            + self.joined_outputs
            + ". "
            + self.prompt_inout_inf
            + " "
            + self.prompt_other_inf
            + "\nNo extra explanation. Must be valid JSON with 'algorithm' and 'code'."
        )
        return prompt_content

    # Similarly adapt get_prompt_m2, m3, h1, f1, c1, p1
    # Each ensures LLM responds in JSON with "algorithm" & "code".

    def get_prompt_m2(self, indiv1: dict[str, str]) -> str:
        prompt_content = (
            self.prompt_task
            + "\n"
            + "Current algorithm + code:\n"
            + indiv1["algorithm"]
            + "\n"
            + indiv1["code"]
            + "\n\n"
            + "Identify the main parameters and create a new algorithm with different parameter settings.\n\n"
            + "Return your answer in **valid JSON**:\n"
            + """
{
  "algorithm": "...",
  "code": "..."
}
"""
            + "Implement function "
            + self.prompt_func_name
            + " with "
            + str(len(self.prompt_func_inputs))
            + " inputs: "
            + self.joined_inputs
            + ", returning "
            + str(len(self.prompt_func_outputs))
            + " outputs: "
            + self.joined_outputs
            + ". "
            + self.prompt_inout_inf
            + " "
            + self.prompt_other_inf
            + "\nNo extra explanation. Must be valid JSON with 'algorithm' and 'code'."
        )
        return prompt_content

    def get_prompt_m3(self, indiv1: dict[str, str]) -> str:
        prompt_content = (
            self.prompt_task
            + "\n"
            + "Below is the code to be simplified:\n"
            + indiv1["code"]
            + "\n\n"
            "Simplify it to avoid overfitting, keeping function name, inputs, outputs.\n\n"
            "Return your answer in **valid JSON**:\n"
            + """
{
  "algorithm": "...",
  "code": "..."
}
"""
            + self.prompt_inout_inf
            + "\nNo extra explanation. Must be valid JSON with 'algorithm' and 'code'."
        )
        return prompt_content

    def get_prompt_h1(self, indivs: list[dict[str, str]]) -> str:
        prompt_indiv = ""
        for i, indiv in enumerate(indivs):
            prompt_indiv += (
                f"No.{i + 1} algorithm + code:\n"
                + indiv["algorithm"]
                + "\n"
                + indiv["code"]
                + "\n"
            )

        prompt_content = (
            self.prompt_task
            + "\n"
            + "You have multiple algorithms:\n"
            + prompt_indiv
            + "Please create a NEW hybrid algorithm by combining strengths.\n\n"
            + "Return your answer in **valid JSON**:\n"
            + """
{
  "algorithm": "...",
  "code": "..."
}
"""
            + "Implement function "
            + self.prompt_func_name
            + " with "
            + str(len(self.prompt_func_inputs))
            + " inputs, returning "
            + str(len(self.prompt_func_outputs))
            + " outputs. "
            + self.prompt_inout_inf
            + " "
            + self.prompt_other_inf
            + "\nNo extra explanation. Must be valid JSON with 'algorithm' and 'code'."
        )
        return prompt_content

    def get_prompt_f1(self, indiv1: dict[str, str], failure_cases: str) -> str:
        prompt_content = (
            self.prompt_task
            + "\n"
            + "We have an algorithm:\n"
            + indiv1["algorithm"]
            + "\n"
            + indiv1["code"]
            + "\n"
            "Failure cases:\n"
            + failure_cases
            + "\n\n"
            + "Improve the algorithm by addressing these failures.\n\n"
            + "Return in **valid JSON**:\n"
            + """
{
  "algorithm": "...",
  "code": "..."
}
"""
            + "Implement function "
            + self.prompt_func_name
            + " with "
            + str(len(self.prompt_func_inputs))
            + " inputs, returning "
            + str(len(self.prompt_func_outputs))
            + " outputs. "
            + self.prompt_inout_inf
            + " "
            + self.prompt_other_inf
            + "\nNo extra explanation. Must be valid JSON."
        )
        return prompt_content

    def get_prompt_c1(self, indiv1: dict[str, str], constraints: str) -> str:
        prompt_content = (
            self.prompt_task
            + "\n"
            + "We have code:\n"
            + indiv1["algorithm"]
            + "\n"
            + indiv1["code"]
            + "\nConstraints:\n"
            + constraints
            + "\n\n"
            + "Optimize the algorithm to handle these constraints.\n\n"
            + "Return in **valid JSON**:\n"
            + """
{
  "algorithm": "...",
  "code": "..."
}
"""
            + "Implement function "
            + self.prompt_func_name
            + " with "
            + str(len(self.prompt_func_inputs))
            + " inputs, returning "
            + str(len(self.prompt_func_outputs))
            + " outputs. "
            + self.prompt_inout_inf
            + " "
            + self.prompt_other_inf
            + "\nNo extra explanation. Must be valid JSON."
        )
        return prompt_content

    def get_prompt_p1(self, indiv1: dict[str, str], complexity_target: str) -> str:
        prompt_content = (
            self.prompt_task
            + "\n"
            + "We have code:\n"
            + indiv1["algorithm"]
            + "\n"
            + indiv1["code"]
            + "\nComplexity target:\n"
            + complexity_target
            + "\n\n"
            "Add complexity layers while preserving interpretability.\n\n"
            "Return in **valid JSON**:\n"
            + """
{
  "algorithm": "...",
  "code": "..."
}
"""
            + "Implement function "
            + self.prompt_func_name
            + " with "
            + str(len(self.prompt_func_inputs))
            + " inputs, returning "
            + str(len(self.prompt_func_outputs))
            + " outputs. "
            + self.prompt_inout_inf
            + " "
            + self.prompt_other_inf
            + "\nNo extra explanation. Must be valid JSON."
        )
        return prompt_content

    ############################################################################
    # Core Response Handling (JSON-based)
    ############################################################################

    def _get_alg(self, prompt_content: str) -> list[str]:
        """
        Internal helper to query the LLM, expecting valid JSON with keys:
            "algorithm" and "code".

        Returns:
            [code_all, algorithm]

        Example math formula (illustrative):
            Let R be the raw string from the LLM, then
              O = json.loads(R)
            => time complexity ~ O(len(R)).

        Args:
            prompt_content: The prompt string to send to the LLM.
        """
        max_retries = 3
        n_retry = 1
        algorithm = ""
        code = ""

        while n_retry <= max_retries:
            response = self.interface_llm.get_response(prompt_content)
            try:
                data = json.loads(response)
                if "algorithm" not in data or "code" not in data:
                    raise ValueError("Missing 'algorithm' or 'code' in JSON.")

                algorithm = data["algorithm"]
                code = data["code"]
                break
            except (json.JSONDecodeError, ValueError) as e:
                if self.debug_mode:
                    print(f"Attempt {n_retry} => invalid JSON or missing keys: {e}")
                # Update the prompt to remind the LLM about the needed format
                prompt_content = (
                    "Your last output was not valid JSON with the required keys. "
                    "Please return valid JSON with keys 'algorithm' and 'code' only."
                )
            n_retry += 1

        # Optionally, merge function outputs if you want
        code_all = code + " " + ", ".join(s for s in self.prompt_func_outputs)
        return [code_all, algorithm]

    ############################################################################
    # Exposed Methods (Operators)
    ############################################################################

    def i1(self) -> list[str]:
        """
        Operator 'i1': Generate an initial algorithm from scratch (no parents).
        Returns [code_all, algorithm].
        """
        prompt_content = self.get_prompt_i1()

        if self.debug_mode:
            print("\n>>> Prompt [i1]:\n", prompt_content)
            input("Press Enter to continue...")

        code_all, algorithm = self._get_alg(prompt_content)

        if self.debug_mode:
            print("\n>>> 'algorithm':\n", algorithm)
            print(">>> 'code':\n", code_all)
            input("Press Enter to continue...")

        return [code_all, algorithm]

    def e1(self, parents: list[dict[str, str]]) -> list[str]:
        """
        Operator 'e1': Generate a new algorithm totally different from the given parents'.
        """
        prompt_content = self.get_prompt_e1(parents)

        if self.debug_mode:
            print("\n>>> Prompt [e1]:\n", prompt_content)
            input("Press Enter to continue...")

        code_all, algorithm = self._get_alg(prompt_content)
        if self.debug_mode:
            print("\n>>> 'algorithm':\n", algorithm)
            print(">>> 'code':\n", code_all)
            input("Press Enter to continue...")

        return [code_all, algorithm]

    def e2(self, parents: list[dict[str, str]]) -> list[str]:
        """
        Operator 'e2': Generate a new algorithm that is different yet
        motivated by the common backbone idea of the parents.
        """
        prompt_content = self.get_prompt_e2(parents)

        if self.debug_mode:
            print("\n>>> Prompt [e2]:\n", prompt_content)
            input("Press Enter to continue...")

        code_all, algorithm = self._get_alg(prompt_content)
        if self.debug_mode:
            print("\n>>> 'algorithm':\n", algorithm)
            print(">>> 'code':\n", code_all)
            input("Press Enter to continue...")

        return [code_all, algorithm]

    def m1(self, parents: dict[str, str]) -> list[str]:
        """
        Operator 'm1': Create a new algorithm as a modified version of 'parents'.
        """
        prompt_content = self.get_prompt_m1(parents)

        if self.debug_mode:
            print("\n>>> Prompt [m1]:\n", prompt_content)
            input("Press Enter to continue...")

        code_all, algorithm = self._get_alg(prompt_content)
        if self.debug_mode:
            print("\n>>> 'algorithm':\n", algorithm)
            print(">>> 'code':\n", code_all)
            input("Press Enter to continue...")

        return [code_all, algorithm]

    def m2(self, parents: dict[str, str]) -> list[str]:
        """
        Operator 'm2': Adjust parameters of the parent's algorithm.
        """
        prompt_content = self.get_prompt_m2(parents)

        if self.debug_mode:
            print("\n>>> Prompt [m2]:\n", prompt_content)
            input("Press Enter to continue...")

        code_all, algorithm = self._get_alg(prompt_content)
        if self.debug_mode:
            print("\n>>> 'algorithm':\n", algorithm)
            print(">>> 'code':\n", code_all)
            input("Press Enter to continue...")

        return [code_all, algorithm]

    def m3(self, parents: dict[str, str]) -> list[str]:
        """
        Operator 'm3': Simplify the code to enhance generalization.
        """
        prompt_content = self.get_prompt_m3(parents)

        if self.debug_mode:
            print("\n>>> Prompt [m3]:\n", prompt_content)
            input("Press Enter to continue...")

        code_all, algorithm = self._get_alg(prompt_content)
        if self.debug_mode:
            print("\n>>> 'algorithm':\n", algorithm)
            print(">>> 'code':\n", code_all)
            input("Press Enter to continue...")

        return [code_all, algorithm]

    def h1(self, parents: list[dict[str, str]]) -> list[str]:
        """
        Operator 'h1': Create a new hybrid algorithm by combining multiple parents.
        """
        prompt_content = self.get_prompt_h1(parents)

        if self.debug_mode:
            print("\n>>> Prompt [h1]:\n", prompt_content)
            input("Press Enter to continue...")

        code_all, algorithm = self._get_alg(prompt_content)
        if self.debug_mode:
            print("\n>>> 'algorithm':\n", algorithm)
            print(">>> 'code':\n", code_all)
            input("Press Enter to continue...")

        return [code_all, algorithm]

    def f1(self, parent: dict[str, str], failure_cases: str) -> list[str]:
        """
        Operator 'f1': Improve the algorithm by analyzing known failure cases.
        """
        prompt_content = self.get_prompt_f1(parent, failure_cases)

        if self.debug_mode:
            print("\n>>> Prompt [f1]:\n", prompt_content)
            input("Press Enter to continue...")

        code_all, algorithm = self._get_alg(prompt_content)
        if self.debug_mode:
            print("\n>>> 'algorithm':\n", algorithm)
            print(">>> 'code':\n", code_all)
            input("Press Enter to continue...")

        return [code_all, algorithm]

    def c1(self, parent: dict[str, str], constraints: str) -> list[str]:
        """
        Operator 'c1': Create a constraint-optimized algorithm.
        """
        prompt_content = self.get_prompt_c1(parent, constraints)

        if self.debug_mode:
            print("\n>>> Prompt [c1]:\n", prompt_content)
            input("Press Enter to continue...")

        code_all, algorithm = self._get_alg(prompt_content)
        if self.debug_mode:
            print("\n>>> 'algorithm':\n", algorithm)
            print(">>> 'code':\n", code_all)
            input("Press Enter to continue...")

        return [code_all, algorithm]

    def p1(self, parent: dict[str, str], complexity_target: str) -> list[str]:
        """
        Operator 'p1': Increase complexity of the parent algorithm, given a target.
        """
        prompt_content = self.get_prompt_p1(parent, complexity_target)

        if self.debug_mode:
            print("\n>>> Prompt [p1]:\n", prompt_content)
            input("Press Enter to continue...")

        code_all, algorithm = self._get_alg(prompt_content)
        if self.debug_mode:
            print("\n>>> 'algorithm':\n", algorithm)
            print(">>> 'code':\n", code_all)
            input("Press Enter to continue...")

        return [code_all, algorithm]
